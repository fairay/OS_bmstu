# Лабораторная работа №8, VFS

* [Построчный разбор программы](additional/comment.c)
* [Порядок выполнения](#порядок-выполнения)
* [Разбор функций](#практика)
* [Основные понятия](#теория)
* [Ссылочки](#источники)

## Порядок выполнения

1. Написание упрощённой версии (без кэширования indoe через kmem_cache).
2. Написание усложнённой версии (с кэшированием).

Что вообще делать с этим?
По методичке, для любой версии программы можно выполнить монтирование:

```console
// собираем наш загружаемый модуль (в данном случае его также называют драйвером ФС)
# make

// загрузка модуля
# sudo insmod ./vfs_md.ko 

// создание образа диска image (по сути, данная команда создаст пустой текстовый файл image)
# touch image

// создание каталога dir, который станет точкой монтирования
# mkdir dir

// монтирование файловой системы
// -o loop - будет использован драйвер "диска", данные будут записываться в образ
// -t myfs - имя используемой ФС = myfs. То есть будет использована ФС созданая в нашем модуле, имя "myfs" указывается в myfs_type.
// ./image - устройство расположения ФС
// ./dir - каталог монтирования ФС (корневой каталог ФС)  
# sudo mount -o loop -t myfs ./image ./dir

// просматриваем логи ядра (должны увидеть сообщения о успешной загрузке модуля и успешном монтировании)
# dmesg | tail -10

//
// Вот в этот момент можно зайти в dir и попробовать разные станадртные комманды (ls, mkdir, ...).
// Так как в myfs_super_ops рабочий только деструктор суперблока, то тут нельзя ничего.
//

// размонтирование ФС
# sudo umount ./dir

// выгрузка модуля
# sudo rmmod ./vfs_md.ko 

// и ещё раз просматриваем логи ядра
# dmesg | tail -10

```

В версии с кэшированием также будет логированна информация о кэшировании (количество объектов, размер и количество вызовов конструктора co).  
Причём число вызовов конструктора далеко не равно количеству объектов (31 объект и 256 вызовов), что объясняется необходимостью переразмещения существующих элементов или перераспределением данных между процессорами.  

Теперь эксперимент от меня:

* number=2, число вызовов = 256
* number=31, число вызовов = 256 (изначальные условия)
* number=256, число вызовов = 512
* number=300, число вызовов = 512
* number=1000, число вызовов = 1024

Тенденция тут понятная - число вызовов конструктора почему-то всегда кратно 256.

``` console
// Для версии с кэшированием также можно вызвать следующую команду для просмотра информации про my_cache
// Формат вывода: # name   <active_objs> <num_objs> <objsize> ... 

# cat /proc/slabinfo | grep my_cache
my_cache        256    256     16  256    1 : tunables    0    0    0 : slabdata      1      1      0

```

## Практика

**register_filesystem(&myfs_type)** - системный вызов регистрации ФС.  
**unregister_filesystem(&myfs_type)** - системный вызов дерегистрации ФС.  
В обоих случаях используется указатель на структуру file_system_type, которая описывает ФС.

``` C
// mount a filesystem residing on a block device
struct dentry *mount_bdev(struct file_system_type *fs_type, int flags, const char *dev_name, void *data, int (*fill_super)(struct super_block*, void*, int))

// fs_type - структура, описывающая ФС
// dev_name - имя ФС
// fill_super - функция инициализации суперблока
```

### Slab-кэши

``` C
// Создание нового слаба
struct kmem_cache *kmem_cache_create(const char *name, size_t size, size_t offset, unsigned long flags, void (*ctor)(void*));  

// name - имя кэша
// size - размер элементов
// offset - смещение первого элемента от начала кэша
// flags - параметры
// ctor - конструктор, вызывается при размещении каждого элемента

// Уничтожение слаба (код ошибки указывает на вид утечки памяти)
int kmem_cache_destroy(kmem_cache_t *cache)

// Выделение объектов кэша
void* kmem_cache_alloc(kmem_cache_t *cache, int kmflag)

// Освобождение объектов кэша
void kmem_cache_free( kmem_cache_t *cache, const void *obj);
// obj - указатель на освобождаемый объект

```

## Теория

**Файловая система** - порядок, определяющий способ организации, хранения и именования данных.  

**VFS (виртуальная файловая система)** - уровень абстракции опр-ию POSIX API, который освобождает пользователя от знания каждой ФС.  
Системные вызовы open/read/write/close работают одинаково вне зависимости от того, какая ФС расположена ниже.

4 основные структуры VFS:

### **struct superblock**

Файл, хранящий информацию о ФС (то есть, это контейнер для метаданных высокого уровня).  
Данная структура существует на диске (для надёжности в нескольких местах), а также в памяти ядра (туда она попадает в момент монтирования ФС).  

### **struct dentry**

directory entry - элемент каталога (компоненты пути).  
`Пример: /mnt/foo.txt - имя пути, из которого VFS выделит компоненты пути '/' (корневой каталог), 'mnt' (каталог), 'foo.txt' (обычный файл)`  
Структура не хранится на диске (поэтому в ней флагов изменения объекта) и создаётся VFS на основании строкового представления имени пути. Объекты dentry кэшируются ядром с помощью slab.

### **struct inode**

Содержит информацию о файле (самую разную: права доступа, inode_operations, ссылка на суперблок, размер...).  
Любой файл имеет ровно 1 inode, но >=1 имён (за счёт hardlink).
Хранится на диске (дисковый inode) и в памяти (inode ядра). Они описывают один и тот же файл, но ядрёный - информацию акутальную для ядра, а дисковый - информацию для адресации данных файла.

### **struct file**

Структура открытого файла. Хранит в себе ссылку на dentry, file_operations, f_pos - позицию ввода-вывода, ...  
Также не хранится на диске.

В суперблоке также содержится указатель на:

* **struct super_operations** - описывает функции над суперблоком: read_inode, write_inode...
* **struct dentry_operations** -описывает функциии над dentry: d_init, d_delete, d_input, ....

**Монтирование ФС** -  системный процесс, подготавливающий раздел диска к использованию операционной системой.

## Источники

1. <http://opennet.ru/base/dev/virtual_fs.txt> - то, на что ссылается сама Рязань в методичке. По сути - настоящий кладезь.
2. <http://www.opennet.ru/docs/RUS/lki/lki-3.html>
